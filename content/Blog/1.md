---
title: "Advances in 3D Perception, Foundation Robotics Models, Safe AI, and Edge Intelligence"
tags: ["computer vision", "3D perception", "foundation models", "scene understanding", "human-robot interaction", "edge computing", "robotics", "AI safety"]
# author: ["Mojtaba Mozaffar"]
description: "An overview of my research activities at Amazon Robotics, focusing on unified 3D perception methods, foundation robotics models, safety‑certifiable AI for human‑robot interaction, and efficient edge AI solutions."
summary: "At Amazon Robotics, I led research efforts on 3D perception and scene understanding AI for robotic workcells; designed safety‑certifiable AI stacks for guaranteed safe human‑robot collaboration; engineered highly efficient AI inference frameworks to run advanced models on edge devices; and contributed to the development of large robotic foundation models that can reason about a wide range of working condition, tasks, and embodiments."
# cover:
#     image: "ar.jpeg"
#     alt: "Manipulation at Amazon Robotics"
#     relative: false
---

<img src="/ar.jpeg" width="800">


As Senior Applied Scientist at Amazon Robotics, I have led and contributed to several reserach efforts for robot manipulation tasks:

1. **Unified 3D Perception & Scene Understanding**  
   Developed and deployed an ML‑based depth estimation pipeline encompassing monocular, stereo, and multi‑view stereo methods. By leveraging commodity RGB cameras and novel fusion architectures, we achieve millimeter‑level accuracy—over 10× improvement compared to legacy active‑sensor solutions.

2. **Safety‑Certifiable AI for Human‑Robot Interaction**  
   Architected a multi‑modal sensing stack combined with formal verification techniques to guarantee safe operation around people. This safety‑certifiable AI framework underpins daily human‑robot collaboration without incidents.

3. **Foundation Models for Robotics**
    Contributed to a team effort training a vision‑language‑action model on millions of simulated and real-world manipulation trajectories. Leveraging a unified encoder for both visual observations and textual task descriptions, the model exhibits strong zero‑shot generalization to novel pick‑and‑place and navigation tasks without additional fine‑tuning.

4. **Efficient Edge AI for Robotics Applications**  
   Engineered optimized inference engines and model compression workflows (TensorRT, JAX, PyTorch quantization) to run advanced perception and action models on edge compute hardware. Our solutions maintain real‑time performance under strict resource constraints, enabling large‑scale rollout of intelligent robots in space‑ and power‑limited environments.

Due to the proprietary and non‑public nature of these projects, detailed results are published through Amazon‑internal conferences and patent filings. You can find the technologies I worked on in the following public annoucements:

+ [Amazon Introduces Sparrow: A State‑of‑the‑Art Robot](/news/operations/amazon‑introduces‑sparrow‑a‑state‑of‑the‑art‑robot‑that‑handles‑millions‑of‑diverse‑products)  
+ [GeekWire: Inside Amazon’s Next‑Gen Robot](https://www.youtube.com/watch?v=sBxKiz5I11o&ab_channel=GeekWire)  
+ [Amazon Robotics at Work in Fulfillment Centers](/news/operations/amazon‑robotics‑robots‑fulfillment‑center)  
+ [The Hindu Business Line: Amazon Robotics Deep Dive](https://www.youtube.com/watch?v=L8a-jt3Kcus&ab_channel=TheHindubusinessline)

---

